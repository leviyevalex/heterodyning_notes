%!TEX root = main.tex

\section{Relative binning}
\subsection{Motivation}
In GW data analysis, we work with a complex inner product space $(\com^n, \brk[a]{\cdot,\cdot}_{\text{GW}})$, where for $h,g \in \com^n$, $\brk[a]{\cdot,\cdot}_{\text{GW}}:\com^n \times \com^n \to \com$ is a sesquilinear inner product defined by:
\begin{equation}\label{eq:gw-inner}
\brk[a]{h,g}_{\text{GW}} \defn 4 \sum_{i=1}^n \frac{\tilde{h}(f_i)^* \tilde{g}(f_i)}{S(f_i)} \Delta f,
\end{equation}
where $\Delta f = 1/T$, where $T$ is the duration of the signal, $\tilde{h} \defn \text{DFT}(h)$, and $S(f)$ is a function which weighs each contribution to the sum (called the \textit{power spectral density.})
Recall that a sesquilinear inner product satisfies the following three axioms:
\begin{itemize}
  \item $\brk[a]{f,g} = \brk[a]{g,f}^*,$ \hfill{(conjugate symmetry)}
  \item $\brk[a]{f, g + \alpha h} = \brk[a]{f,g} + \alpha \brk[a]{f,h}$, \hfill{(linearity in second argument)}
  \item $\brk[a]{h,h} > 0$ for $h \neq 0_{\com^n}$. \hfill{(postive definiteness)}
  % \item $\brk[a]{h,h} \gt 0$ and $\brk[a]{h,h} = 0 \iff h = 0_{\com^n}$. \hfill{(postive definiteness)}
\end{itemize}
\begin{remark}[]\label{}
In physics it is typical to require linearity in the second argument. Hence we suggest sticking to this convention.
\end{remark}
The likelihood is defined in terms of the inner product by
\begin{equation}\label{}
\like(\theta) \defn e^{-\frac{1}{2} \re \brk[a]1{h(\cdot ; \theta) - d(\cdot),h(\cdot ; \theta) - d(\cdot)}_{\text{GW}}},
\end{equation}
where $d \in \com^n$ is the detector strain, and $h$ is a waveform model parameterized by $\theta$ (e.g, IMRPhenomD).
Using the linearity and conjugate symmetry of the inner product, we may show that\footnote{We use the notation $h(\cdot, \theta)$ to indicate a $\com^n$ vector paramerized by $\theta$.
In the following, we suppress the first slot which indexes the vector when convenient.}
\begin{align*}\label{}
-\ln \like (\theta) &= \frac{1}{2} \re \brk[a]1{h(\theta) - d,h(\theta) - d}_{\text{GW}} \\
&= \frac{1}{2}\overbrace{\brk[a]{h(\theta), h(\theta)}_{\text{GW}}}^{(a)} - \re \overbrace{\brk[a]{h(\theta), d}_{\text{GW}}}^{(b)} + \frac{1}{2}\overbrace{\brk[a]{d, d}_{\text{GW}}}^{\text(c)}.
\end{align*}
Hence, for every $\like(\theta)$ evaluation, we must evaluate $(a)$ and $(b)$, which require $\order(n)$ evaluations of the waveform.
Note that $(c)$ can be precomputed once and stored.
Relative binning aims to reduce the number of evaluations to $\order(m)$, where $m \ll n$, providing significant computational advantage.

% \subsection{Derivatives and Fisher matrix}

\subsection{The heterodyning strategy}
Heterodyning is a technique in signal processing which combines two high frequency signals to yield another signal with lower frequency.
If we can modify the integrand of \cref{eq:gw-inner} such that it turns from a highly oscillating function into a slowly oscillating one, then we may in principle use a coarser grid in the sum (yielding computational benefits) while accumulating a minimal amount of error.

Consider the following.
Suppose that $\theta_0$ is a fiducial set of parameters, and that we are interested in a signal with support $[\fmin, \fmax]$. Then we define the \textit{heterodyned signal} as
\begin{align*}
r(f;\theta) &\defn \frac{h(f;\theta)}{h(f; \theta_0)} \\
&= \frac{A}{A_0} e^{i \overbrace{\brk[r]1{\psi(f;\theta) - \psi(f;\theta_0)}}^{\defn \Psi(f;\theta)}},
\end{align*}
where $\Psi(f;\theta)$ is referred to as the \textit{differential phase}.
This expression is convenient since it decouples oscillations in the envelope $A/A_0$ and broad oscillations controlled by the phasor.
Indeed, this function lives up to its name if both the envelope and differential phase are ``slowly varying" functions with respect to $f$.
\begin{example}[TaylorF2]\label{}
In the simple case of TaylorF2, the envelope $A/A_0$ is not a function of frequency. On the other hand, it may be shown that the differential phase evolves as a polynomial. Hence, we expect the oscillatory behavior of $r$ to be much milder than that of the original strain $h$.
\end{example}
\begin{remark}[]\label{}
Since $r$ is parameterized by $\theta$, this slow variation condition must hold for a reasonably large subset of parameter space $\Theta$ in order to be useful.
However, analytically verifying this condition is inconvenient as it depends on the particular waveform family used to generate $h$.
This assumption is therefore usually justified in retrospect by numerical experiements.
\end{remark}

In \cref{sec:bin-algorithm}, we will describe a scheme to construct a substantially sparser grid (compared to the original one) to automatically resolve $r$.




If this condition holds, then one may efficiently compress the real and imaginary parts of $r$ with a \textit{linear spline}.
% More complex version suggested in the paper
% \begin{definition}[]\label{}
% Let $r:[\fmin,\fmax] \to \reals$, and consider the cover $[\fmin, \fmax] = \cup_{i=1}^{m-1}[f_-^{(i)}, f_+^{(i)}) \cup [f_-^{(m)}, f^{(m)}_+]$, where $f_+^{(i)}=f_-^{(i+1)}$ for every $i \in \brk[c]{1, \ldots, m-1}$, with $f_-^{(1)}=\fmin$ and $ f_+^{(m)}=\fmax$. Then the \textit{linear spline approximation} of $r$ is given by
% \begin{equation}\label{}
% r(f; \theta) \approx r_0^{(b)}(\theta) + r_1^{(b)}(\theta)(f-\bar{f}^{(b)}), \; \; f \in \text{bin} \; b
% \end{equation}
% where
% \begin{align*}
% r_0^{(b)}(\theta) &= \frac{r(f_+^{(b)};\theta) + r(f_-^{(b)}; \theta)}{2}, &
% r_1^{(b)}(\theta) &= \frac{r(f_+^{(b)};\theta) - r(f_-^{(b)}; \theta)}{f_+^{(b)} - f_-^{(b)}}, &
% \bar{f}^{(b)} &\defn \frac{f^{(b)}_+ + f^{(b)}_-}{2}.
% \end{align*}
% \end{definition}
% Simpler version proposed by me
\begin{definition}[]\label{}
Let $r:[\fmin,\fmax] \to \com$, and consider the cover $[\fmin, \fmax] = \cup_{i=1}^{m-1}[f_-^{(i)}, f_+^{(i)}) \cup [f_-^{(m)}, f^{(m)}_+]$, where $f_+^{(i)}=f_-^{(i+1)}$ for every $i \in \brk[c]{1, \ldots, m-1}$, with $f_-^{(1)}=\fmin$ and $ f_+^{(m)}=\fmax$. Then the \textit{linear spline approximation} of $r$ is given by
\begin{equation}\label{}
r(f; \theta) \approx r_0^{(b)}(\theta) + r_1^{(b)}(\theta)(f-f_-^{(b)}), \; \; f \in \text{bin} \; b
\end{equation}
where
\begin{align*}
r_0^{(b)}(\theta) &\defn r(f_-^{(b)};\theta) , &
r_1^{(b)}(\theta) &\defn \frac{r(f_+^{(b)};\theta) - r(f_-^{(b)}; \theta)}{f_+^{(b)} - f_-^{(b)}}, &
% \bar{f}^{(b)} &\defn \frac{f^{(b)}_+ + f^{(b)}_-}{2}.
\end{align*}
\end{definition}



%
\begin{proposition}[]\label{}
Suppose $r(f;\theta)$ is successfully heterodyned with fiducial parameters $\theta_0$.
Then with a first order spline the following approximations hold
\begin{align*}
\brk[a]{h(\cdot, \theta), d(\cdot)} &\approx \sum_b \brk[s]1{A_0^{(b)} r_0^{(b)}(\theta)^* + A_1^{(b)} r_1^{(b)}(\theta)^*}, \\
% \brk[a]{d(\cdot), h(\cdot, \theta)} &\approx \sum_b \brk[s]1{A_0^{(b)} r_0^{(b)}(\theta)^* + A_1^{(b)} r_1^{(b)}(\theta)^*}, \\
\brk[a]{h(\cdot,\theta), h(\cdot,\theta)} &\approx \sum_b \brk[s]2{B_0^{(b)} \abs{r_0^{(b)}(\theta)}^2 + 2 B_1^{(b)} \re \brk[r]1{r_0^{(b)}(\theta)^* r_1^{(b)}(\theta)}},
\end{align*}
where for every bin $b$, the coefficients $A_0^{(b)}, A_1^{(b)} \in \com$ and $B_0^{(b)}, B_1^{(b)} \in \reals$ are defined by
\begin{align*}
A_0^{(b)} &\defn 4 \sum_{i: f_i \in \text{bin} \; b} \frac{ h(f_i;\theta_0)^* d(f_i) }{S(f_i)} \Delta f, & A_1^{(b)} &= 4 \sum_{i:f_i \in \text{bin} \; b} \frac{ h(f_i;\theta_0)^*d(f_i) (f_i - f_-^{(b)})}{S(f_i)} \Delta f,
\end{align*}
%
\begin{align*}
B_0^{(b)} &\defn 4 \sum_{i:f_i \in \text{bin} \; b} \frac{\abs{h(f_i;\theta_0)}^2}{S(f_i)}\Delta f, & B_1^{(b)} &\defn 4 \sum_{i:f_i \in \text{bin} \: b} \frac{\abs{h(f_i;\theta_0)}^2(f_i-f_-^{(b)})}{S(f_i)}\Delta f.
\end{align*}
\end{proposition}
\begin{proof}
(i) We have
\begin{align*}
\brk[a]{h(\cdot, \theta), d(\cdot)} &= \brk[a]{h(\cdot,\theta_0) r(\cdot,\theta), d(\cdot)}\\
&\defn 4 \sum_{b} \sum_{i:f_i \in \text{bin} \; b} \frac{h(f_i;\theta_0)^* r(f_i;\theta)^* d(f_i) }{S(f_i)}\Delta f \\
&\approx 4 \sum_{b} \sum_{i:f_i \in \text{bin} \; b} \frac{h(f_i;\theta_0)^* d(f_i) }{S(f_i)} \brk[s]!{r_0^{(b)}(\theta)^* + r_1^{(b)}(\theta)^*(f_i - f_-^{(b)})} \Delta f\\
&= \sum_b \brk[s]!{A_0^{(b)} r_0^{(b)}(\theta)^* + A_1^{(b)} r_1^{(b)}(\theta)^*}.
\end{align*}
(ii) On the other hand, we have
\begin{align*}
\brk[a]{h(\cdot,\theta), h(\cdot,\theta)} &= \brk[a]{h(\cdot,\theta_0) r(\cdot,\theta), h(\cdot,\theta_0) r(\cdot,\theta)}\\
% &= 4 \sum_{i=1}^n \frac{\abs{h(f_i;\theta)}^2}{S(f_i)} \Delta f \\
% &= 4 \sum_{i=1}^n \frac{\abs{h(f_i;\theta_0)r(f_i;\theta) }^2}{S(f_i)}\Delta f \\
&\approx 4 \sum_b \sum_{i:f_i \in \text{bin} \; b} \frac{\abs{h(f_i;\theta_0)}^2 \abs1{r_0^{(b)}(\theta) + r_1^{(b)}(\theta) (f_i-f_-^{(b)})}^2}{S(f_i)}\Delta f \\
&= 4 \sum_b \sum_{i:f_i \in \text{bin} \; b}\frac{\abs{h(f_i;\theta_0)}^2 \brk[r]2{\abs{r_0^{(b)}(\theta)}^2 + \abs{r_1^{(b)}(\theta)(f-f_-^{(b)})}^2 + 2(f_i-f_-^{(b)}) \re \brk[r]1{r_0^{(b)}(\theta)^*r_1^{(b)}(\theta)}}}{S(f_i)}\Delta f \\
&\approx \sum_b \brk[s]2{B_0^{(b)} \abs{r_0^{(b)}(\theta)}^2 + 2B_1^{(b)} \re \brk[r]1{r_0^{(b)}(\theta)^* r_1^{(b)}(\theta)}},
\end{align*}
where in the second to last line we've used the fact that for $z_1, z_2 \in \com$, $\abs{z_1+z_2}^2 = \abs{z_1}^2 + \abs{z_2}^2 + 2 \re \brk[r]{z_1^* z_2}$, and in the last line we've ignored terms of $\order(\abs{f-f_-^{(b)}}^2)$.
\end{proof}

\subsection{Heterodyning the gradient and Fisher}
We may also investigate whether whether heterodyning produces accurate gradient and Fisher matrix evaluations.
The expressions for this are given by
\begin{align*}
- \ln \like_{,i}(\theta) &= \re \brk[a]{h_{,i}(\cdot, \theta), h(\cdot, \theta)} - \re \brk[a]{h_{,i}(\cdot, \theta),d(\cdot)} \pushright{\text{(heterodyne)}} \\
&= \re \brk[a]{h_{,i}(\cdot, \theta), h(\cdot, \theta)-d(\cdot)}, \pushright{\text(standard)}
\end{align*}
and
\begin{align*}
- \ln \like_{,ij}(\theta) &= \re \brk[a]{h_{,ij}(\cdot, \theta), h(\cdot,\theta) - d(\cdot)} + \re \brk[a]{h_{,i}(\cdot,\theta), h_{,j}(\cdot, \theta)} \\
&\approx  \re \brk[a]{h_{,i}(\cdot,\theta), h_{,j}(\cdot, \theta)}.
\end{align*}
We define
\begin{equation}\label{eq:fisher-mat}
\Gamma_{ij}(\theta) \defn \re \brk[a]{h_{,i}(\cdot,\theta), h_{,j}(\cdot, \theta)}
\end{equation}
as the \textit{Fisher information matrix}.
From the previous derivation, we see that the Fisher matrix acts as a surrogate for the Hessian of the potential.
In fact, we may use it as a replacement in Newton optimization procedures due to the following
\begin{proposition}[]\label{}
The Fisher information matrix $\Gamma$ is positive-semi definite.
\end{proposition}
\begin{proof}
Let $h(f;\theta) = h_1(f;\theta) + i h_2(f;\theta)$. Then by definition, we have
\begin{align*}
\Gamma_{ij}(\theta) &\defn \re \brk[a]{h_{,i}(\cdot,\theta), h_{,j}(\cdot, \theta)} \\
&= 4 \re \sum_i \frac{h_{,i}(f;\theta)^* h_{,j}(f;\theta)}{S(f_i)} \Delta f \\
&= 4 \re \sum_i \frac{(h_{1,i} - ih_{2,i})(h_{1,j} + ih_{2,j})}{S(f_i)} \Delta f \\
&= 4 \re \sum_i \frac{h_{1,i} h_{1,j} + h_{2,i} h_{2,j} + i(h_{1,i} h_{2,j} - h_{2,i} h_{1,j})}{S(f_i)} \Delta f \\
&= 4 \sum_i \frac{h_{1,i} h_{1,j} + h_{2,i} h_{2,j}}{S(f_i)} \Delta f.
\end{align*}
Since $S>0$ and both matricies in the numerator as PSD, so too is the sum.
\end{proof}


Recall that $h(f; \theta) = A(f; \theta) e^{i \psi(f;\theta)}$, and observe that
\begin{align*}
r_{,j}(f;\theta) &\defn \frac{h_{,j}(f;\theta)}{h(f;\theta_0)} \\
&= \frac{A_{,j}(f;\theta) e^{i \psi(f;\theta)} + i \psi_{,j}(f;\theta) A(f;\theta) e^{i \psi(f;\theta)}}{A(f;\theta_0) e^{i \psi(f;\theta_0)}} \\
&= \frac{A_{,j}(f;\theta)}{A(f;\theta_0)} e^{i \brk[r]{\psi(f;\theta) - \psi(f;\theta_0)}} + \frac{\psi_{,j}(f;\theta)A(f;\theta)}{A(f;\theta_0)}e^{i \brk[r]{\psi(f;\theta) - \psi(f;\theta_0) + \frac{\pi}{2}}}, \\
% &= \alpha_j(f;\theta) + i \beta_j(f;\theta).
\end{align*}
and should also be compatible with our binning scheme.
We now go a step further and derive heterodyned evaluation of the gradient of the potential, and the Fisher matrix.
\begin{proposition}[]\label{}
The terms we need to calculate for the derivative and Fisher may be approximated as follows:
\begin{align*}
\brk[a]{h_{,j}(\theta),d} &\approx \sum_b \brk[s]!{A_0^{(b)} r_{0,j}^{(b)} (\theta)^* + A_1^{(b)} r_{1,j}^{(b)}(\theta)^*}, \\
\brk[a]{h_{,j}(\theta), h(\theta)} &\approx \sum_b \brk[c]2{B_0^{(b)} r_{0,j}^{(b)}(\theta)^* r_0^{(b)}(\theta) + B_1^{(b)} \brk[s]!{r_{0,j}^{(b)}(\theta)^* r_1^{(b)}(\theta) + r_{1,j}^{(b)}(\theta)^* r_0^{(b)}(\theta)}}, \\
\brk[a]{h_{,j}(\theta), h_{,k}(\theta)} &\approx \sum_b \brk[c]2{B_0^{(b)} r_{0,j}^{(b)}(\theta)^* r_{0,k}^{(b)}(\theta) + B_1^{(b)} \brk[s]!{r_{0,j}^{(b)}(\theta)^* r_{1,k}^{(b)}(\theta) + r_{1,j}^{(b)}(\theta)^* r_{0,k}^{(b)}(\theta)}}.
\end{align*}
\end{proposition}
\begin{proof}
(i) Follows immediately from the previous result:
\begin{align*}
\brk[a]{h_{,j}(\theta), d} &= \brk[a]{h(\theta), d}_{,j} \\
&\approx \sum_b \brk[s]1{A_0^{(b)} r_{0,j}^{(b)}(\theta)^* + A_1^{(b)} r_{1,j}^{(b)}(\theta)^*}.
\end{align*}
(ii) Similarly,
\begin{align*}
\brk[a]{h_{,j}(\theta), h(\theta)} &= \brk[a]{h(\cdot,\theta_0) r_{,j}(\cdot,\theta), h(\cdot,\theta_0) r(\cdot,\theta)}\\
% &= 4 \sum_{i=1}^n \frac{h_{,j}(f_i;\theta)^* h(f;\theta)}{S(f_i)} \Delta f \\
% &= 4 \sum_{i=1}^n \frac{r_{,j}(f_i;\theta)^* h(f_i;\theta_0)^* r(f_i;\theta) h(f_i;\theta_0)}{S(f_i)} \Delta f \\
% &= 4 \sum_{i=1}^n \frac{\abs{h(f_i;\theta_0)}^2}{S(f_i)}  r_{,j}(f_i;\theta)^* r(f_i;\theta) \Delta f\\
&\approx 4 \sum_b \sum_{i:f_i \in \text{bin } b}\frac{\abs{h(f_i;\theta_0)}^2}{S(f_i)}\Delta f \brk[s]!{r_{0,j}^{(b)}(\theta) + r_{1,j}^{(b)}(f_i-f_-^{(b)})}^* \brk[s]!{r_{0}^{(b)}(\theta) + r_{1}^{(b)}(f_i-f_-^{(b)})} \\
&\approx \sum_b \brk[c]2{B_0^{(b)} r_{0,j}^{(b)}(\theta)^* r_0^{(b)}(\theta) + B_1^{(b)} \brk[s]!{r_{0,j}^{(b)}(\theta)^* r_1^{(b)}(\theta) + r_{1,j}^{(b)}(\theta)^* r_0^{(b)}(\theta)}},
\end{align*}
where in the last line we've ignored terms of $\order(\abs{f_i - f_-^{(b)}}^2)$.

(iii) Follows a similar proof to (ii). Answer may be read off by replacing $r_0^{(b)}, r_1^{(b)} \leftarrow r_{0,k}^{(b)}, r_{1,k}^{(b)}$ in the rightmost square brackets of the second to last line.
\end{proof}
\subsection{Reduced variance scheme}
\cite{Cornish_2021} suggests using the following
\begin{proposition}[]\label{}
The following expression holds:
\begin{equation}\label{}
- \ln \like(\theta) = -\ln \like(\theta_0) + \frac{1}{2} \re \brk[a]{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta) - h(\cdot,\theta_0)} + \re \brk[a]{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta_0) - d(\cdot)}.
\end{equation}
\end{proposition}
\begin{proof}
The result follows directly from the definition
\begin{align*}
-\ln \like(\theta) &\defn \frac{1}{2} \re \brk[a]{h(\cdot,\theta) - d(\cdot), h(\cdot,\theta)-d(\cdot)} \\
&= \frac{1}{2} \re \brk[a]1{\brk[s]!{h(\cdot,\theta) - h(\cdot,\theta_0)} + \brk[s]!{h(\cdot, \theta_0) - d(\cdot)}, \brk[s]!{h(\cdot, \theta) - h(\cdot,\theta_0)} + \brk[s]!{h(\cdot,\theta_0) - d(\cdot)}} \\
&= \frac{1}{2} \brk[s]2{\re \brk[a]1{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta) - h(\cdot,\theta_0)} + 2 \re \brk[a]1{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta_0) - d(\cdot)}} - \ln \like (\theta_0) \\
&= -\ln \like(\theta_0) + \frac{1}{2} \re \brk[a]{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta) - h(\cdot,\theta_0)} + \re \brk[a]{h(\cdot,\theta) - h(\cdot,\theta_0), h(\cdot,\theta_0) - d(\cdot)}.
\end{align*}
\end{proof}
This appears to be a good expression to approximate the likelihood from because both integrals now have small values, and hence the total error is smaller (i.e, the variance of the overall estimate should be improved).
This idea can be extended to the idea of derivatives as well
\begin{corollary}[]\label{}
The derivative may be expressed as
\begin{equation}\label{}
- \ln \like(\theta)_{,j} = \re \brk[a]{h_{,j}(\cdot,\theta), h(\cdot,\theta) - h(\cdot, \theta_0)} + \re \brk[a]{h_{,j}(\cdot,\theta), h(\cdot,\theta_0) - d(\cdot)}.
\end{equation}
\end{corollary}
\begin{proof}
Follows immediately from the previous result.
\end{proof}
Observe that this is the end of the line.
The Fisher matrix does not benefit from a reduced variance form following this line of reasoning.
Since we are currently using a zero-noise injection, the second term is exactly zero, and hence all we need to focus on in the first term.

\section{Bin selection algorithm}\label{sec:bin-algorithm}
\begin{definition}[]\label{}
Let $h(f;\theta)$ be a complex valued signal. Then the \textit{post-Newtonian ansatz} asserts that $\psi(f)\defn\text{arg}(h(f))$ takes the form
\begin{equation}\label{}
\psi(f;\theta) \defn \sum_i \alpha_i(\theta) f^{\gamma_i},
\end{equation}
where $\gamma \defn (-5/3, -2/3, 1, 5/3, 7/3)$.
\end{definition}
We now have that
\begin{align*}
\Psi(f;\theta) &\defn \psi(f;\theta) - \psi(f; \theta_0) \\
&= \sum_i \brk[r]1{\alpha_i (\theta) - \alpha_i(\theta_0)} f^{\gamma_i} \\
&= \sum_i \delta \alpha_i (\theta) f^{\gamma_i}
\end{align*}
The heterodyne approximation is valid if $\Psi$ varies slowly within a bin, which translates to a constraint on the $\delta \alpha_i$ terms. This motivates the following definition:
\begin{proposition}[]\label{}

Suppose that the parameters $\theta$ are restricted to
\begin{equation}\label{eq:bound-params}
\Theta \defn \brk[c]1{\theta \in \chi : \forall i \; \abs{\delta \alpha_i(\theta)} \le 2 \pi \chi f_{*,i}^{-\gamma_i}},
\end{equation}
where
\begin{equation}\label{}
f_{*,i} \defn
\begin{cases}
f_{\text{max}} & \text{if } \gamma_i > 0 \\
f_{\text{min}} & \text{else}.
\end{cases}
\end{equation}
Then the differential phase change over the interval $[f_-, f_+]$ obeys the following bound:
%change in bin $b \in \brk[c]{1, 2, \ldots, m}$ is bounded by
\begin{equation}\label{}
\abs{\Psi(f_+;\theta) - \Psi(f_-;\theta)} \le 2 \pi \chi \sum_i \abs2{\brk[r]2{\frac{f_+}{f_{*,i}}}^{\gamma_i} - \brk[r]2{\frac{f_-}{f_{*,i}}}^{\gamma_i}}.
\end{equation}
\end{proposition}
% \tag*{\llap{(triangle inequality)}}
\begin{proof}
\begin{align*}
\abs{\Psi(f_+;\theta) - \Psi(f_-;\theta)} &= \abs2{\sum_i \delta \alpha_i(\theta) \brk[s]1{f_+^{\gamma_i} - f_-^{\gamma_i}}} \\
&\le \sum_i \abs{\delta \alpha_i(\theta)} \abs{f_+^{\gamma_i} - f_-^{\gamma_i}} \pushright{(\text{triangle inequality})} \\
&\le 2 \pi \chi \sum_i \abs2{\brk[r]2{\frac{f_+}{f_{*,i}}}^{\gamma_i} - \brk[r]2{\frac{f_-}{f_{*,i}}}^{\gamma_i}}.
\end{align*}
\end{proof}





% Suppose that the parameters $\theta$ are restricted to
% \begin{equation}\label{eq:bound-params}
% \Theta \defn \brk[c]1{\theta \in \chi : \forall i \; \abs{\delta \alpha_i(\theta)} \le 2 \pi \chi f_{*,i}^{-\gamma_i}},
% \end{equation}
% where
% \begin{equation}\label{}
% f_{*,i} \defn
% \begin{cases}
% f_+ & \text{if } \gamma_i > 0 \\
% f_- & \text{else}.
% \end{cases}
% \end{equation}
% Then the differential phase change over the interval $[f_-, f_+]$ is bounded by
% %change in bin $b \in \brk[c]{1, 2, \ldots, m}$ is bounded by
% \begin{equation}\label{}
% \abs{\Psi(f_+;\theta) - \Psi(f_-;\theta)} \le 2 \pi \chi \sum_i \brk[s]2{1 - \brk[r]2{\frac{f_-}{f_+}}^{\abs{\gamma_i}}}.
% \end{equation}
% \end{proposition}
% % \tag*{\llap{(triangle inequality)}}
% \begin{proof}
% \begin{align*}
% \abs{\Psi(f_+;\theta) - \Psi(f_-;\theta)} &= \abs2{\sum_i \delta \alpha_i(\theta) \brk[s]1{f_+^{\gamma_i} - f_-^{\gamma_i}}} \\
% &\le \sum_i \abs{\delta \alpha_i(\theta)} \abs{f_+^{\gamma_i} - f_-^{\gamma_i}} \pushright{(\text{triangle inequality})} \\
% &\le 2 \pi \chi \sum_i \abs2{\brk[r]2{\frac{f_+}{f_{*,i}}}^{\gamma_i} - \brk[r]2{\frac{f_-}{f_{*,i}}}^{\gamma_i}} \\
% &= 2 \pi \chi \brk[s]3{\sum_{i:\gamma_i>0} \abs2{1 - \brk[r]2{\frac{f_-}{f_+}}^{\gamma_i}} + \sum_{i:\gamma_i<0} \abs2{\brk[r]2{\frac{f_+}{f_-}}^{\gamma_i} - 1}} \\
% &= 2 \pi \chi \brk[s]3{\sum_{i:\gamma_i>0} \abs2{1 - \brk[r]2{\frac{f_-}{f_+}}^{\abs{\gamma_i}}} + \sum_{i:\gamma_i<0} \abs2{1-\brk[r]2{\frac{f_-}{f_+}}^{\abs{\gamma_i}}}} \\
% &= 2 \pi \chi \sum_i \abs2{1 - \brk[r]2{\frac{f_-}{f_+}}^{\abs{\gamma_i}}} \\
% &= 2 \pi \chi \sum_i \brk[s]2{1-\brk[r]2{\frac{f_-}{f_+}}^{\abs{\gamma_i}}}.
% \end{align*}
% \end{proof}
